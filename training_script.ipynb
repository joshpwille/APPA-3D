{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b5102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Reward: -122.10000000000005, Epsilon: 0.99\n",
      "Episode 2/1000, Reward: -120.50000000000003, Epsilon: 0.99\n",
      "Episode 3/1000, Reward: -109.09999999999998, Epsilon: 0.99\n",
      "Episode 4/1000, Reward: -103.7, Epsilon: 0.98\n",
      "Episode 5/1000, Reward: -111.39999999999998, Epsilon: 0.98\n",
      "Episode 6/1000, Reward: -121.20000000000003, Epsilon: 0.97\n",
      "Episode 7/1000, Reward: -104.5, Epsilon: 0.97\n",
      "Episode 8/1000, Reward: -119.30000000000001, Epsilon: 0.96\n",
      "Episode 9/1000, Reward: -105.5, Epsilon: 0.96\n",
      "Episode 10/1000, Reward: -108.1, Epsilon: 0.95\n",
      "Episode 11/1000, Reward: -107.19999999999999, Epsilon: 0.95\n",
      "Episode 12/1000, Reward: -110.29999999999998, Epsilon: 0.94\n",
      "Episode 13/1000, Reward: -131.20000000000016, Epsilon: 0.94\n",
      "Episode 14/1000, Reward: -113.19999999999997, Epsilon: 0.93\n",
      "Episode 15/1000, Reward: -118.6, Epsilon: 0.93\n",
      "Episode 16/1000, Reward: -124.80000000000008, Epsilon: 0.92\n",
      "Episode 17/1000, Reward: -103.8, Epsilon: 0.92\n",
      "Episode 18/1000, Reward: -105.8, Epsilon: 0.91\n",
      "Episode 19/1000, Reward: -110.69999999999997, Epsilon: 0.91\n",
      "Episode 20/1000, Reward: -109.59999999999998, Epsilon: 0.90\n",
      "Episode 21/1000, Reward: -118.39999999999999, Epsilon: 0.90\n",
      "Episode 22/1000, Reward: -126.90000000000012, Epsilon: 0.90\n",
      "Episode 23/1000, Reward: -113.99999999999997, Epsilon: 0.89\n",
      "Episode 24/1000, Reward: -122.40000000000005, Epsilon: 0.89\n",
      "Episode 25/1000, Reward: -137.50000000000026, Epsilon: 0.88\n",
      "Episode 26/1000, Reward: -109.19999999999999, Epsilon: 0.88\n",
      "Episode 27/1000, Reward: -141.1000000000003, Epsilon: 0.87\n",
      "Episode 28/1000, Reward: -107.99999999999999, Epsilon: 0.87\n",
      "Episode 29/1000, Reward: -109.59999999999998, Epsilon: 0.86\n",
      "Episode 30/1000, Reward: -123.50000000000006, Epsilon: 0.86\n",
      "Episode 31/1000, Reward: -104.3, Epsilon: 0.86\n",
      "Episode 32/1000, Reward: -109.59999999999998, Epsilon: 0.85\n",
      "Episode 33/1000, Reward: -111.29999999999998, Epsilon: 0.85\n",
      "Episode 34/1000, Reward: -117.99999999999999, Epsilon: 0.84\n",
      "Episode 35/1000, Reward: -109.69999999999999, Epsilon: 0.84\n",
      "Episode 36/1000, Reward: -128.50000000000014, Epsilon: 0.83\n",
      "Episode 37/1000, Reward: -112.09999999999997, Epsilon: 0.83\n",
      "Episode 38/1000, Reward: -118.29999999999998, Epsilon: 0.83\n",
      "Episode 39/1000, Reward: -120.30000000000001, Epsilon: 0.82\n",
      "Episode 40/1000, Reward: -113.69999999999996, Epsilon: 0.82\n",
      "Episode 41/1000, Reward: -107.1, Epsilon: 0.81\n",
      "Episode 42/1000, Reward: -106.89999999999999, Epsilon: 0.81\n",
      "Episode 43/1000, Reward: -106.0, Epsilon: 0.81\n",
      "Episode 44/1000, Reward: -110.99999999999997, Epsilon: 0.80\n",
      "Episode 45/1000, Reward: -104.7, Epsilon: 0.80\n",
      "Episode 46/1000, Reward: -142.60000000000034, Epsilon: 0.79\n",
      "Episode 47/1000, Reward: -122.30000000000004, Epsilon: 0.79\n",
      "Episode 48/1000, Reward: -124.30000000000007, Epsilon: 0.79\n",
      "Episode 49/1000, Reward: -106.8, Epsilon: 0.78\n",
      "Episode 50/1000, Reward: -107.99999999999999, Epsilon: 0.78\n",
      "Episode 51/1000, Reward: -140.3000000000003, Epsilon: 0.77\n",
      "Episode 52/1000, Reward: -118.1, Epsilon: 0.77\n",
      "Episode 53/1000, Reward: -114.49999999999997, Epsilon: 0.77\n",
      "Episode 54/1000, Reward: -115.49999999999996, Epsilon: 0.76\n",
      "Episode 55/1000, Reward: -110.99999999999997, Epsilon: 0.76\n",
      "Episode 56/1000, Reward: -113.59999999999997, Epsilon: 0.76\n",
      "Episode 57/1000, Reward: -127.00000000000011, Epsilon: 0.75\n",
      "Episode 58/1000, Reward: -109.79999999999998, Epsilon: 0.75\n",
      "Episode 59/1000, Reward: -106.69999999999999, Epsilon: 0.74\n",
      "Episode 60/1000, Reward: -108.49999999999999, Epsilon: 0.74\n",
      "Episode 61/1000, Reward: -111.19999999999997, Epsilon: 0.74\n",
      "Episode 62/1000, Reward: -123.60000000000007, Epsilon: 0.73\n",
      "Episode 63/1000, Reward: -121.40000000000003, Epsilon: 0.73\n",
      "Episode 64/1000, Reward: -130.70000000000016, Epsilon: 0.73\n",
      "Episode 65/1000, Reward: -113.39999999999996, Epsilon: 0.72\n",
      "Episode 66/1000, Reward: -110.59999999999998, Epsilon: 0.72\n",
      "Episode 67/1000, Reward: -126.3000000000001, Epsilon: 0.71\n",
      "Episode 68/1000, Reward: -107.49999999999999, Epsilon: 0.71\n",
      "Episode 69/1000, Reward: -101.8, Epsilon: 0.71\n",
      "Episode 70/1000, Reward: -103.60000000000001, Epsilon: 0.70\n",
      "Episode 71/1000, Reward: -129.20000000000016, Epsilon: 0.70\n",
      "Episode 72/1000, Reward: -113.79999999999997, Epsilon: 0.70\n",
      "Episode 73/1000, Reward: -111.69999999999997, Epsilon: 0.69\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define the UAV environment (Custom)\n",
    "class UAVEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(UAVEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(4)  # Example: 4 actions (up, down, left, right)\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32)  # UAV state: x, y, speed, angle\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset UAV position and environment\n",
    "        self.state = np.array([0, 0, 1, 0])  # Example: [x, y, speed, angle]\n",
    "        self.goal = np.array([10, 10])  # Goal position (target)\n",
    "        self.done = False\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update UAV state based on action\n",
    "        if action == 0:  # Move up\n",
    "            self.state[1] += 1\n",
    "        elif action == 1:  # Move down\n",
    "            self.state[1] -= 1\n",
    "        elif action == 2:  # Move left\n",
    "            self.state[0] -= 1\n",
    "        elif action == 3:  # Move right\n",
    "            self.state[0] += 1\n",
    "\n",
    "        # Calculate distance to goal (reward)\n",
    "        dist_to_goal = np.linalg.norm(self.state[:2] - self.goal)\n",
    "        if dist_to_goal < 1.0:\n",
    "            reward = 100  # Reached goal\n",
    "            self.done = True\n",
    "        else:\n",
    "            # Penalize the UAV for each time step (encouraging efficient movement)\n",
    "            reward = -0.1\n",
    "\n",
    "        # Check for collision with boundaries or obstacles (simple example)\n",
    "        if self.state[0] < -10 or self.state[0] > 10 or self.state[1] < -10 or self.state[1] > 10:\n",
    "            reward = -100  # Collision penalty\n",
    "            self.done = True\n",
    "\n",
    "        return np.array(self.state), reward, self.done, {}\n",
    "\n",
    "# Define the Q-Network (Deep Q-Learning Model)\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Define the experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Training parameters\n",
    "state_dim = 4  # [x, y, speed, angle]\n",
    "action_dim = 4  # 4 possible actions\n",
    "batch_size = 32\n",
    "gamma = 0.99  # Discount factor\n",
    "learning_rate = 0.001\n",
    "epsilon_start = 1.0  # Epsilon-greedy exploration parameter\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.995  # Epsilon decay rate\n",
    "num_episodes = 1000\n",
    "buffer_capacity = 10000\n",
    "target_update_frequency = 10  # Update the target network every 10 episodes\n",
    "\n",
    "# Initialize environment, model, and optimizer\n",
    "env = UAVEnv()\n",
    "model = QNetwork(state_dim, action_dim)\n",
    "target_model = QNetwork(state_dim, action_dim)\n",
    "target_model.load_state_dict(model.state_dict())  # Target model is a copy of the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Initialize experience replay buffer\n",
    "buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "# Training loop\n",
    "epsilon = epsilon_start\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        if random.random() < epsilon:\n",
    "            action = env.action_space.sample()  # Random action (exploration)\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            q_values = model(state_tensor)\n",
    "            action = torch.argmax(q_values).item()  # Greedy action (exploitation)\n",
    "\n",
    "        # Take action and observe the next state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # Sample a batch from the replay buffer\n",
    "        if buffer.size() >= batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.bool)\n",
    "\n",
    "            # Q-learning target\n",
    "            next_q_values = target_model(next_states)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            targets = rewards + (gamma * max_next_q_values) * (~dones)\n",
    "\n",
    "            # Get current Q-values from the model\n",
    "            q_values = model(states)\n",
    "            current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(current_q_values, targets)\n",
    "\n",
    "            # Backpropagate and update the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "    # Periodically update the target model\n",
    "    if episode % target_update_frequency == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Log episode results\n",
    "    print(f\"Episode {episode+1}/{num_episodes}, Reward: {episode_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"uav_dqn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045c476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
