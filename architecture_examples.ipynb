{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7c2e9c7-f8e6-4be1-8cf8-35b06627b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import matplotlib\n",
    "matplotlib.use('qt5agg')  # Or 'tkagg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96209d94-aadb-43de-9002-aaa366cb4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69ed7c44-60b3-4340-8329-9a93b08509c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aff5a28e-ddb4-457c-b19a-3c5b8936bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Climb Rate: 2.62 ft/s\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "k = 1.0  # Attractive potential scaling constant\n",
    "m = 1.0  # Repulsive potential scaling constant\n",
    "rho_0 = 10.0  # Obstacle influence radius\n",
    "epsilon = 8.8e-12  # Small constant to prevent division by zero\n",
    "step_size = 5.0  # Movement step size for the UAV\n",
    "max_steps = 500  # Maximum number of steps\n",
    "\n",
    "DEBUG_LIMIT = 5  # Limit the number of debug prints\n",
    "\n",
    "# Start and Target positions\n",
    "uav_x, uav_y, uav_z = 50, 400, 750\n",
    "endpoint_x, endpoint_y, endpoint_z = 350, 1, 2100\n",
    "UAV_position_initial = ([uav_x, uav_y, uav_z]) \n",
    "UAV_position = np.array([uav_x, uav_y, uav_z])  # Start position\n",
    "goal_position = np.array([endpoint_x, endpoint_y, endpoint_z])  # Goal position\n",
    "obstacle_positions = [(100.0, 200.0, 1000.0), (280.0, 70.0, 1500.0)]\n",
    "no_fly_zones = [\n",
    "    {\"x_min\": 100, \"x_max\": 150, \"y_min\": 200, \"y_max\": 250, \"z_min\": 900, \"z_max\": 1100},\n",
    "    {\"x_min\": 250, \"x_max\": 300, \"y_min\": 100, \"y_max\": 150, \"z_min\": 1300, \"z_max\": 1600},\n",
    "]\n",
    "\n",
    "# UAV Dynamics Class\n",
    "class UAVDynamics:\n",
    "    def __init__(self, weight, sref, thrust_available):\n",
    "        self.weight = weight  # UAV weight in lb\n",
    "        self.sref = sref      # Reference wing area in ft²\n",
    "        self.thrust_available = thrust_available  # Available thrust in lb\n",
    "        self.cd = 0.05  # Drag coefficient (assumed constant)\n",
    "\n",
    "    def compute_climb_rate(self, rho, velocity):\n",
    "        q = 0.5 * rho * velocity**2  # Dynamic pressure\n",
    "        drag = q * self.sref * self.cd  # Drag force\n",
    "        climb_rate = (self.thrust_available - drag) / self.weight  # ft/s\n",
    "        return climb_rate\n",
    "\n",
    "# Parameters\n",
    "weight = 43  # UAV weight in lb\n",
    "sref = 5.67  # Reference wing area in ft²\n",
    "thrust_available = 113  # Maximum thrust available in lb\n",
    "rho = 0.0023769  # Air density at sea level in slugs/ft³\n",
    "velocity = 38.2  # Airspeed in ft/s\n",
    "safety_buffer = 100  # 10 ft above terrain as a safety margin\n",
    "\n",
    "# Create UAV Dynamics Instance\n",
    "uav = UAVDynamics(weight, sref, thrust_available)\n",
    "climb_rate = uav.compute_climb_rate(rho, velocity)\n",
    "print(f\"Maximum Climb Rate: {climb_rate:.2f} ft/s\")\n",
    "\n",
    "################# initilize q(s,a) ##########################\n",
    "actions = {\n",
    "    0: (0, 0, 1),   # Up\n",
    "    1: (0, 0, -1),  # Down\n",
    "    2: (-1, 0, 0),  # Left\n",
    "    3: (1, 0, 0),   # Right\n",
    "    4: (0, 1, 0),   # Forward\n",
    "    5: (0, -1, 0),  # Backward\n",
    "    6: (0, 1, 1),   # Diagonal Up-Forward\n",
    "    7: (0, -1, 1),  # Diagonal Up-Backward\n",
    "    8: (-1, 0, 1),  # Diagonal Up-Left\n",
    "    9: (1, 0, 1),  # Diagonal Up-Right\n",
    "    10: (0, 1, -1), # Diagonal Down-Forward\n",
    "    11: (0, -1, -1),# Diagonal Down-Backward\n",
    "    12: (-1, 0, -1),# Diagonal Down-Left\n",
    "    13: (1, 0, -1), # Diagonal Down-Right\n",
    "    14: (-1, 1, 0), # Diagonal Left-Forward\n",
    "    15: (-1, -1, 0),# Diagonal Left-Backward\n",
    "    16: (1, 1, 0),  # Diagonal Right-Forward\n",
    "    17: (1, -1, 0)  # Diagonal Right-Backward\n",
    "}\n",
    "\n",
    "# Define state and action space\n",
    "num_actions = 18  # Total number of actions\n",
    "initial_probabilities = {action: 1.0 / num_actions for action in actions}  # Uniform distribution\n",
    "\n",
    "states = [(x, y, z) for x in range(50) for y in range(50) for z in range(10)]  # Example 3D grid\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = {}\n",
    "\n",
    "# Set initial values for all states and actions\n",
    "initial_value = 0.0\n",
    "for s in states:\n",
    "    Q[s] = {a: initial_value for a in actions}\n",
    "    \n",
    "# Define terminal state\n",
    "terminal_state = (endpoint_x, endpoint_y, endpoint_z)  # Example terminal state\n",
    "\n",
    "# Set Q(terminal-state, a) = 0 for all actions\n",
    "Q[terminal_state] = {a: 0.0 for a in actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b068c77e-783a-4d01-bab7-8a56f965036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terrain_elevation(x, y):\n",
    "    x_idx = np.clip(int(x), 0, smoothed_elevation.shape[1] - 1)\n",
    "    y_idx = np.clip(int(y), 0, smoothed_elevation.shape[0] - 1)\n",
    "    return smoothed_elevation[y_idx, x_idx]\n",
    "\n",
    "def is_in_no_fly_zone(position, no_fly_zones):\n",
    "    for zone in no_fly_zones:\n",
    "        if (zone[\"x_min\"] <= position[0] <= zone[\"x_max\"] and\n",
    "            zone[\"y_min\"] <= position[1] <= zone[\"y_max\"] and\n",
    "            zone[\"z_min\"] <= position[2] <= zone[\"z_max\"]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Function to compute the attractive potential\n",
    "def compute_attractive_potential(position, goal):\n",
    "    d_goal = np.linalg.norm(position - goal) + epsilon\n",
    "    return 0.5 * k * d_goal**2\n",
    "\n",
    "# Function to compute the attractive force\n",
    "def compute_attractive_force(position, goal):\n",
    "    return k * (goal - position)\n",
    "\n",
    "# Function to compute the repulsive potential\n",
    "def compute_repulsive_potential(position, obstacles):\n",
    "    U_rep = 0.0\n",
    "    for obs in obstacles:\n",
    "        d_barrier = np.linalg.norm(position - obs)\n",
    "        if d_barrier <= rho_0:\n",
    "            d_barrier = max(d_barrier, epsilon)  # Avoid division by zero\n",
    "            U_rep += 0.5 * m * (1 / d_barrier - 1 / rho_0)**2\n",
    "    return U_rep\n",
    "\n",
    "# Function to compute the repulsive force\n",
    "def compute_repulsive_force(position, obstacles):\n",
    "    F_rep = np.array([0.0, 0.0, 0.0])\n",
    "    for obs in obstacles:\n",
    "        d_barrier = np.linalg.norm(position - obs)\n",
    "        if d_barrier <= rho_0:\n",
    "            d_barrier = max(d_barrier, epsilon)  # Avoid division by zero\n",
    "            F_rep += m * (1 / d_barrier - 1 / rho_0) * (1 / d_barrier**2) * (position - obs) / d_barrier\n",
    "    return F_rep\n",
    "\n",
    "# Function to compute individual repulsive forces\n",
    "def compute_repulsive_force_separate(position, obstacles):\n",
    "    F_rep_all = []\n",
    "    for i, obs in enumerate(obstacles):\n",
    "        d_barrier = np.linalg.norm(position - obs)\n",
    "        F_rep = np.array([0.0, 0.0, 0.0])\n",
    "        if d_barrier <= rho_0:\n",
    "            d_barrier = max(d_barrier, epsilon)\n",
    "            F_rep = m * (1 / d_barrier - 1 / rho_0) * (1 / d_barrier**2) * (position - obs) / d_barrier\n",
    "        F_rep_all.append(np.linalg.norm(F_rep))\n",
    "        # Correctly indented debug statement\n",
    "#        print(f\"Obstacle {i+1}: Distance = {d_barrier:.2f}, Force = {np.linalg.norm(F_rep):.2e}\")\n",
    "    return F_rep_all\n",
    "\n",
    "# Function to calculate reward\n",
    "def calculate_reward2(d_goal, d_barrier, d_max, D_cz, D_mz):\n",
    "    reward = 0.0\n",
    "    if d_goal >= D_cz and d_barrier >= D_cz:\n",
    "        R_a = np.tanh((d_max - d_goal) / d_max) * d_goal\n",
    "        reward += R_a\n",
    "    elif D_mz <= d_barrier <= D_cz or D_mz <= d_goal <= D_cz:\n",
    "        R_cz_goal = np.tanh((D_cz - d_goal) / D_cz) * d_goal\n",
    "        R_cz_obstacle = np.tanh((D_cz - d_barrier) / D_cz) * d_barrier\n",
    "        reward += R_cz_goal + R_cz_obstacle\n",
    "    if d_barrier < D_mz:\n",
    "        R_mz = np.tanh((D_mz - d_barrier) / D_mz) * d_barrier\n",
    "        reward += R_mz\n",
    "    return reward\n",
    "\n",
    "# Function to update the UAV position\n",
    "def update_position(position, force, step_size):\n",
    "    return position + step_size * force / np.linalg.norm(force)\n",
    "\n",
    "# Function to check if the UAV has reached the goal\n",
    "def reached_goal(position, goal, tolerance=10.0):\n",
    "    return np.linalg.norm(position - goal) <= tolerance\n",
    "\n",
    "def predict_future_positions(position, force, step_size, steps_ahead=2):\n",
    "    future_positions = []\n",
    "    current_position = position.copy()\n",
    "    normalized_force = force / np.linalg.norm(force)\n",
    "    for _ in range(steps_ahead):\n",
    "        current_position = current_position + step_size * normalized_force\n",
    "        future_positions.append(current_position.copy())\n",
    "    return future_positions\n",
    "\n",
    "def get_terrain_elevations_for_future_positions(future_positions, raster_data):\n",
    "    elevations = []\n",
    "    for pos in future_positions:\n",
    "        terrain_z = get_terrain_elevation(pos[0], pos[1])\n",
    "        elevations.append(terrain_z)\n",
    "    return elevations\n",
    "\n",
    "def calculate_reward(d_goal_t, d_barrier_t, d_max, D_cz, D_mz, X_t, X_g, a_t, obstacles, \n",
    "                     Ra=1.0, Rcz=1.0, Rmz=1.0, critical_distance=10, obstacle_penalty=2.0, goal_weight=2.0):\n",
    "    X_t_plus1 = X_t + a_t\n",
    "    d_goal_t_plus1 = np.linalg.norm(X_t_plus1 - X_g)\n",
    "    d_barrier_t_plus1 = min(np.linalg.norm(X_t_plus1 - obs) for obs in obstacles)\n",
    "    \n",
    "    Δdistance_to_goal = d_goal_t - d_goal_t_plus1\n",
    "    Δdistance_to_obstacle = d_barrier_t_plus1 - d_barrier_t\n",
    "\n",
    "    direction_goal = Δdistance_to_goal / abs(Δdistance_to_goal) if Δdistance_to_goal != 0 else 0\n",
    "    direction_barrier = Δdistance_to_obstacle / abs(Δdistance_to_obstacle) if Δdistance_to_obstacle != 0 else 0\n",
    "\n",
    "    reward = 0.0\n",
    "    if Δdistance_to_goal < 0:  # Moving farther from the goal\n",
    "        reward -= 10  # Strong penalty for moving away\n",
    "    else:  # Moving closer to the goal\n",
    "        reward += goal_weight * Δdistance_to_goal\n",
    "    if d_barrier_t < critical_distance:  # Within critical obstacle zone\n",
    "        penalty = obstacle_penalty * (critical_distance - d_barrier_t) / critical_distance\n",
    "        reward -= penalty\n",
    "    if d_goal_t >= D_cz and d_barrier_t >= D_cz:\n",
    "        R_a = np.tanh((d_max - d_goal_t) / d_max) * direction_goal\n",
    "        reward += Ra * R_a\n",
    "    if D_mz <= d_barrier_t <= D_cz or D_mz <= d_goal_t <= D_cz:\n",
    "        R_cz_goal = np.tanh((D_cz - d_goal_t) / D_cz) * direction_goal\n",
    "        R_cz_obstacle = np.tanh((D_cz - d_barrier_t) / D_cz) * direction_barrier\n",
    "        reward += Rcz * (R_cz_goal + R_cz_obstacle)\n",
    "    if d_barrier_t < D_mz:\n",
    "        R_mz = -np.tanh((D_mz - d_barrier_t) / D_mz) * direction_barrier\n",
    "        reward += Rmz * R_mz\n",
    "    return reward\n",
    "\n",
    "def select_action_epsilon_greedy(Q, actions, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(actions)  # Random exploration\n",
    "    else:\n",
    "        return actions[np.argmax(Q)]  # Exploitation\n",
    "\n",
    "def select_action_softmax(Q, current_state, actions, tau):\n",
    "    Q_values = np.array([Q[current_state, action] for action in actions])\n",
    "\n",
    "    # Prevent overflow in exponential\n",
    "    Q_clipped = np.clip(Q, -100, 100)  # Clip to avoid overflow\n",
    "    exp_Q = np.exp(Q_clipped / tau)  # Compute exponentials safely\n",
    "    probs = exp_Q / np.sum(exp_Q)  # Normalize to get probabilities\n",
    "\n",
    "    # Check if probabilities contain NaN\n",
    "    if np.any(np.isnan(probs)):\n",
    "        print(f\"Original Q-values: {Q}\")\n",
    "        print(f\"Clipped Q-values: {Q_clipped}\")\n",
    "        print(f\"Rescaled Q-values: {Q_rescaled}\")\n",
    "        print(f\"Exponential values: {exp_Q}\")\n",
    "        print(f\"Probabilities: {probs}\")\n",
    "        raise ValueError(\"Softmax probabilities contain NaN values\")\n",
    "    action_idx = np.random.choice(range(len(actions)), p=probs)  # Select based on probabilities\n",
    "    return actions[action_idx], action_idx\n",
    "\n",
    "\n",
    "def update_action_probabilities(P, Q, epsilon=0.1):\n",
    "    num_actions = len(Q)\n",
    "    # Identify indices of actions with the maximum Q-value\n",
    "    best_action_indices = np.argwhere(Q == np.max(Q)).flatten()\n",
    "    m = len(best_action_indices)  # Number of optimal actions\n",
    "    # Start with uniform distribution\n",
    "    updated_P = np.full(num_actions, epsilon / num_actions)\n",
    "    # Distribute remaining probability among optimal actions\n",
    "    remaining_probability = 1.0 - epsilon\n",
    "    for i in best_action_indices:\n",
    "        updated_P[i] += remaining_probability / m\n",
    "    return updated_P\n",
    "\n",
    "tau = 1.0\n",
    "# Initialize uniform probabilities based on Eq. (19)\n",
    "def initialize_uniform_probabilities(actions):\n",
    "    num_actions = len(actions)  # |A_s| - total actions in the action set\n",
    "    uniform_prob = 1.0 / num_actions  # Equal probability for each action\n",
    "    return {action: uniform_prob for action in actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfd9083c-e9ca-49a5-ba9d-3668e8bc4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkWithCNN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetworkWithCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1)  # Conv1D for 1D input\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1)  # Second Conv1D\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "\n",
    "        # Adaptive pooling\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 64)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 128)  # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(128, action_size)  # Final layer outputs a single value... action_size = 18\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.bn1(torch.relu(self.conv1(state)))\n",
    "        x = self.pool(x)\n",
    "        x = self.bn2(torch.relu(self.conv2(x)))\n",
    "        x = self.pool(x).squeeze(-1)  # Flatten the pooled features\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)  # Output Q-values for each action\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Value and advantage streams\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Outputs the value of the state\n",
    "        )\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size)  # Outputs the advantages of each action\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.feature(state)\n",
    "        value = self.value(features)\n",
    "        advantage = self.advantage(features)\n",
    "        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        return q_values\n",
    "\n",
    "import torchvision.models as models\n",
    "class ResNetDQN(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(ResNetDQN, self).__init__()\n",
    "        # Use a pretrained ResNet backbone\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove classification layer\n",
    "        \n",
    "        # Add a fully connected layer for action-value prediction\n",
    "        self.fc = nn.Linear(512, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        features = self.resnet(state)\n",
    "        q_values = self.fc(features)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class RNNQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(RNNQNetwork, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=state_size, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        rnn_out, _ = self.rnn(state)\n",
    "        q_values = self.fc(rnn_out[:, -1, :])  # Use the last time step's output\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_size),  # Outputs action probabilities\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Outputs state value\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        shared_out = self.shared(state)\n",
    "        action_probs = self.actor(shared_out)\n",
    "        state_value = self.critic(shared_out)\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "class TransformerQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(TransformerQNetwork, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=state_size, nhead=4)\n",
    "        self.transformer = TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.fc = nn.Linear(state_size, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state.permute(1, 0, 2)  # Transformer expects [seq_len, batch_size, features]\n",
    "        transformer_out = self.transformer(state)\n",
    "        q_values = self.fc(transformer_out[-1])  # Use the last time step\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d282bc67-0378-4d1b-b3a7-f1e23f4b307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Action Probabilities for the State: {0: 0.05555555555555555, 1: 0.05555555555555555, 2: 0.05555555555555555, 3: 0.05555555555555555, 4: 0.05555555555555555, 5: 0.05555555555555555, 6: 0.05555555555555555, 7: 0.05555555555555555, 8: 0.05555555555555555, 9: 0.05555555555555555, 10: 0.05555555555555555, 11: 0.05555555555555555, 12: 0.05555555555555555, 13: 0.05555555555555555, 14: 0.05555555555555555, 15: 0.05555555555555555, 16: 0.05555555555555555, 17: 0.05555555555555555}\n"
     ]
    }
   ],
   "source": [
    "# Example for one state\n",
    "state_action_probabilities = initialize_uniform_probabilities(actions)\n",
    "print(\"Initial Action Probabilities for the State:\", state_action_probabilities)\n",
    "\n",
    "# Load raster data\n",
    "raster = rasterio.open(r'C:\\Users\\joshp\\OneDrive\\Desktop\\CPP Fall 24\\eVTOL\\python code\\evironment data\\USGS_san_gab_mtns.tif')\n",
    "elevation_data = raster.read(1)  # Read the elevation data\n",
    "\n",
    "# Downsample the data by a factor of 25\n",
    "downsample_factor = 25\n",
    "elevation_data_downsampled = elevation_data[::downsample_factor, ::downsample_factor]\n",
    "\n",
    "# Apply Gaussian smoothing to the downsampled data\n",
    "smoothed_elevation = gaussian_filter(elevation_data_downsampled, sigma=5)\n",
    "\n",
    "# Normalize the elevation data to range 0 to 3.5\n",
    "normalized_elevation = smoothed_elevation / 1000  # Divide by 1000\n",
    "\n",
    "# Define X and Y coordinates for the downsampled data\n",
    "x = np.arange(normalized_elevation.shape[1])\n",
    "y = np.arange(normalized_elevation.shape[0])\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Define finer grid for interpolation\n",
    "x_fine = np.linspace(0, normalized_elevation.shape[1] - 1, normalized_elevation.shape[1] * 2)\n",
    "y_fine = np.linspace(0, normalized_elevation.shape[0] - 1, normalized_elevation.shape[0] * 2)\n",
    "x_fine, y_fine = np.meshgrid(x_fine, y_fine)\n",
    "\n",
    "# Interpolate normalized elevation data\n",
    "interpolated_elevation = griddata(\n",
    "    (x.ravel(), y.ravel()), normalized_elevation.ravel(), (x_fine, y_fine), method='cubic'\n",
    ")\n",
    "\n",
    "# Navigation loop\n",
    "trajectory = [UAV_position.copy()]  # Track the UAV's path\n",
    "F_rep_values = []  # Repulsive forces\n",
    "distance_to_first_obstacle = []  # Distance to the first obstacle\n",
    "distance_to_second_obstacle = []  # Distance to the second obstacle\n",
    "F_rep_individual = [[], []]  # One list per obstacle\n",
    "steps = 0\n",
    "\n",
    "# REWARD\n",
    "# Initialize reward tracking\n",
    "rewards = []  # List to store rewards over time\n",
    "\n",
    "# Define constants for reward calculation\n",
    "D_cz = 1000  # Example value for critical zone threshold\n",
    "D_mz = 500  # Example value for minimum zone threshold\n",
    "d_max = np.linalg.norm(UAV_position - goal_position)  # Initial max distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a041f4d9-5195-4e28-9ba5-e4d0463b0b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 450018 samples\n"
     ]
    }
   ],
   "source": [
    "# Initialize data storage\n",
    "data = []\n",
    "for state, actions in Q.items():\n",
    "    if not isinstance(state, (list, tuple, np.ndarray)):\n",
    "        raise TypeError(f\"State must be a list, tuple, or numpy array, got {type(state)}\")\n",
    "    state = np.array(state)\n",
    "    for action, q_value in actions.items():\n",
    "        if not isinstance(action, (int, tuple)):\n",
    "            raise TypeError(f\"Action must be an int or tuple, got {type(action)}\")\n",
    "        if isinstance(action, int):\n",
    "            action = (action,)  # Convert integer action into tuple\n",
    "        data.append((state, np.array(action), q_value))\n",
    "        \n",
    "print(f\"Data size: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0bb6bb9-abfe-4d09-a975-1fe5390cb508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix X shape: (450018, 4)\n",
      "Target vector y shape: (450018,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare input and target arrays\n",
    "X = []  # Input features\n",
    "y = []  # Target Q-values\n",
    "\n",
    "for state, action, q_value in data:\n",
    "    state_action = np.concatenate((state, action))  # Combine state and action\n",
    "    X.append(state_action)\n",
    "    y.append(q_value)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Feature matrix X shape: {X.shape}\")\n",
    "print(f\"Target vector y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "704fa02d-9c52-485c-840a-a164f464cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Convert data to tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = data_utils.TensorDataset(X_tensor, y_tensor)\n",
    "loader = data_utils.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"DataLoader created with batch size: 64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98a093bf-6867-4984-8eb4-06dc5a81adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Q-network with input size 4\n"
     ]
    }
   ],
   "source": [
    "# Initialize network and optimizer\n",
    "state_size = len(next(iter(Q.keys())))  # Assume all states have the same structure\n",
    "\n",
    "first_action = next(iter(Q.values())).keys().__iter__().__next__()\n",
    "if isinstance(first_action, int):  # Single integer action\n",
    "    action_size = 1\n",
    "elif isinstance(first_action, tuple):  # Multi-dimensional tuple action\n",
    "    action_size = len(first_action)\n",
    "else:\n",
    "    raise TypeError(f\"Unsupported action type: {type(first_action)}\")\n",
    "\n",
    "q_network = QNetworkWithCNN(state_size + action_size, 1)  # Network for Q(s,a)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f\"Initialized Q-network with input size {state_size + action_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05a953d3-0bab-49a0-a3e8-ecd5671acdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Using GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "\n",
      "Epoch 1/10\n",
      "Epoch 1 completed. Average Loss: 9.805987023458077e-05\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 2 completed. Average Loss: 1.013640275107724e-07\n",
      "Stopping early due to low loss threshold.\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "min_loss_threshold = 8.8e-6\n",
    "accumulation_steps = 4\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "# Ensure PyTorch uses the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Move the model to the device\n",
    "q_network = QNetworkWithCNN(state_size, action_size).to(device)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "    q_network.train()  # Set the model to training mode (important for batch normalization)\n",
    "    epoch_loss = 0.0  # Track loss for the epoch\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(loader):\n",
    "        # Move batch to the device\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Reshape input to match [batch_size, channels, sequence_length]\n",
    "        batch_x = batch_x.unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = q_network(batch_x)\n",
    "        predictions = predictions.squeeze(-1)\n",
    "        loss = criterion(predictions, batch_y) / accumulation_steps\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Accumulate batch loss\n",
    "        epoch_loss += loss.item() * accumulation_steps\n",
    "\n",
    "    # Calculate average epoch loss\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss}\")\n",
    "\n",
    "    # Check stopping condition\n",
    "    if avg_loss < min_loss_threshold:\n",
    "        print(\"Stopping early due to low loss threshold.\")\n",
    "        break\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "044e975b-00d4-4cda-bb07-ab033aebda1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for sample states: tensor([[-0.0007],\n",
      "        [-0.0007],\n",
      "        [-0.0007],\n",
      "        [-0.0007],\n",
      "        [-0.0007]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Best actions: tensor([0, 0, 0, 0, 0], device='cuda:0')\n",
      "Min reward: 0.0, Max reward: 0.0\n",
      "Final training loss: 1.013640275107724e-07\n"
     ]
    }
   ],
   "source": [
    "q_network.eval()  # Switch to evaluation mode\n",
    "\n",
    "# Evaluate the model on a small batch of data\n",
    "sample_states = X_tensor[:5].to(device)  # Move to the same device as the model\n",
    "\n",
    "# Ensure input has the correct shape\n",
    "sample_states = sample_states.unsqueeze(1)  # Add channel dimension if required\n",
    "\n",
    "# Run the forward pass\n",
    "q_values = q_network(sample_states)\n",
    "print(f\"Q-values for sample states: {q_values}\")\n",
    "\n",
    "# Extract best actions (index of the maximum Q-value for each state)\n",
    "best_actions = torch.argmax(q_values, dim=1)\n",
    "print(f\"Best actions: {best_actions}\")\n",
    "print(f\"Min reward: {torch.min(y_tensor)}, Max reward: {torch.max(y_tensor)}\")\n",
    "print(f\"Final training loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174ee088-f14b-4913-af8b-88a0c0a4ebfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m action_mapping \u001b[38;5;241m=\u001b[39m actions \n\u001b[0;32m      2\u001b[0m terrain_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.1\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'actions' is not defined"
     ]
    }
   ],
   "source": [
    "action_mapping = actions \n",
    "terrain_z = 0\n",
    "dt = float(0.1) \n",
    "\n",
    "while not reached_goal(UAV_position, goal_position) and steps < max_steps:\n",
    "\n",
    "    # Step 1: Current state and Q-values\n",
    "    current_state = UAV_position.copy()\n",
    "    current_state_tensor = torch.tensor(current_state, dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    q_values = q_network(current_state_tensor)\n",
    "    best_action_idx = torch.argmax(q_values, dim=1).item() + 1\n",
    "    best_action = actions.get(best_action_idx, None)\n",
    "    \n",
    "    if best_action is None:\n",
    "        raise ValueError(f\"Invalid action index: {best_action_idx}\")\n",
    "\n",
    "    print(f\"Q-values: {q_values}\")\n",
    "    print(f\"Selected action index: {best_action_idx}\")\n",
    "    print(f\"Selected action: {best_action}\")\n",
    "    print(f\"Updated UAV position: {UAV_position}\")\n",
    "\n",
    "    # Step 2: Compute distances\n",
    "    d_goal = np.linalg.norm(UAV_position - goal_position)\n",
    "    d_barrier = min([np.linalg.norm(UAV_position - obs) for obs in obstacle_positions])\n",
    "\n",
    "    d_goal_t = np.linalg.norm(UAV_position - goal_position)\n",
    "    d_barrier_t = min([np.linalg.norm(UAV_position - obs) for obs in obstacle_positions])\n",
    "\n",
    "    # Step 3: Compute forces\n",
    "    F_att = compute_attractive_force(UAV_position, goal_position)\n",
    "    F_rep = compute_repulsive_force(UAV_position, obstacle_positions)\n",
    "    F_rep_all = compute_repulsive_force_separate(UAV_position, obstacle_positions)\n",
    "\n",
    "    # Add noise to encourage exploration\n",
    "    force_noise = np.random.uniform(-0.1, 0.1, size=F_att.shape)\n",
    "    total_force = F_att + F_rep + force_noise\n",
    "\n",
    "    reward = calculate_reward(\n",
    "    d_goal_t, d_barrier_t, d_max, D_cz, D_mz, \n",
    "    UAV_position, goal_position, total_force, obstacle_positions)\n",
    "\n",
    "    rewards.append(reward)\n",
    "\n",
    "    # Step 4: Terrain and no-fly zone forces\n",
    "    terrain_z = float(get_terrain_elevation(UAV_position[0], UAV_position[1]))\n",
    "    print(f\"Terrain elevation: {terrain_z}, type: {type(terrain_z)}\")\n",
    "    if not isinstance(terrain_z, (int, float)):\n",
    "        raise TypeError(f\"Invalid type for terrain_z: {type(terrain_z)}. Expected int or float.\")\n",
    "\n",
    "    F_rep_terrain = np.array([0.0, 0.0, 0.0])\n",
    "    if UAV_position[2] < terrain_z + safety_buffer:\n",
    "        F_rep_terrain = np.array([0, 0, 1]) * (terrain_z + safety_buffer - UAV_position[2])\n",
    "\n",
    "    # No-fly zone repulsion force\n",
    "    F_rep_no_fly = np.array([0.0, 0.0, 0.0])\n",
    "    if is_in_no_fly_zone(UAV_position, no_fly_zones):\n",
    "        F_rep_no_fly = np.array([0, 0, 1]) * 100  # Strong upward repulsion\n",
    "        reward -= 50  # Penalize no-fly zone entry\n",
    "    \n",
    "    F_total = F_att + F_rep + F_rep_terrain + F_rep_no_fly\n",
    "    \n",
    "    # Step 5: Update UAV position based on action\n",
    "    UAV_position = UAV_position.astype(np.float64)\n",
    "    acceleration = F_total / weight  # F = ma\n",
    "    velocity += acceleration * dt\n",
    "    UAV_position += velocity * dt\n",
    "\n",
    "    # Ensure UAV doesn't collide with terrain\n",
    "    if UAV_position[2] < terrain_z + safety_buffer:\n",
    "        UAV_position[2] = terrain_z + safety_buffer  # Correct altitude\n",
    "        reward -= 50  # Penalize collision with terrain\n",
    "\n",
    "    # Step 6: Calculate reward\n",
    "    reward = calculate_reward(\n",
    "        d_goal, d_barrier, d_max, D_cz, D_mz, \n",
    "        UAV_position, goal_position, total_force, obstacle_positions\n",
    "    )\n",
    "    rewards.append(reward)\n",
    "\n",
    "    # Step 7: Record variables for debugging/analysis\n",
    "    trajectory.append(UAV_position.copy())\n",
    "    F_rep_values.append(np.linalg.norm(F_rep))\n",
    "    distance_to_first_obstacle.append(np.linalg.norm(UAV_position - obstacle_positions[0]))\n",
    "    distance_to_second_obstacle.append(np.linalg.norm(UAV_position - obstacle_positions[1]))\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Step: {steps}, Position: {UAV_position}, Reward: {reward}\")\n",
    "    print(f\"Force components - Attractive: {F_att}, Repulsive: {F_rep}, Terrain: {F_rep_terrain}, No-fly: {F_rep_no_fly}\")\n",
    "\n",
    "    UAV_position += velocity * dt  # Update UAV position\n",
    "    X_t = UAV_position.copy()      # Update X_t to reflect new position\n",
    "\n",
    "    \n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feca139d-b4a0-44d1-910b-162d5527d003",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (500,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, obs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(obstacle_positions):\n\u001b[1;32m---> 73\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(steps), F_rep_individual[i], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepulsive Force from Obstacle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     74\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepulsive Force Magnitude\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3786\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3788\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3793\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3796\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[0;32m   3797\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   3798\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3799\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3800\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1779\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1779\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:296\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    295\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_args(\n\u001b[0;32m    297\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[38;5;241m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:486\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    483\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (500,) and (0,)"
     ]
    }
   ],
   "source": [
    "#torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "\n",
    "# Define X and Y coordinates for the smoothed elevation grid\n",
    "x_smooth = np.arange(smoothed_elevation.shape[1])\n",
    "y_smooth = np.arange(smoothed_elevation.shape[0])\n",
    "x_smooth, y_smooth = np.meshgrid(x_smooth, y_smooth)\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# Add No-Fly Zones to the plot\n",
    "for zone in no_fly_zones:\n",
    "    x_range = [zone[\"x_min\"], zone[\"x_max\"], zone[\"x_max\"], zone[\"x_min\"], zone[\"x_min\"]]\n",
    "    y_range = [zone[\"y_min\"], zone[\"y_min\"], zone[\"y_max\"], zone[\"y_max\"], zone[\"y_min\"]]\n",
    "    z_range = [zone[\"z_min\"]] * 5\n",
    "\n",
    "    ax.plot(x_range, y_range, z_range, color='purple', label='No-Fly Zone', alpha=0.5)\n",
    "\n",
    "    z_top_range = [zone[\"z_max\"]] * 5\n",
    "    ax.plot(x_range, y_range, z_top_range, color='purple', alpha=0.5)\n",
    "\n",
    "    # Connect top and bottom corners\n",
    "    for i in range(4):\n",
    "        ax.plot(\n",
    "            [x_range[i], x_range[i]],\n",
    "            [y_range[i], y_range[i]],\n",
    "            [zone[\"z_min\"], zone[\"z_max\"]],\n",
    "            color='purple', alpha=0.5\n",
    "        )\n",
    "ax.plot_surface(x_smooth, y_smooth, smoothed_elevation,\n",
    "    cmap='terrain', edgecolor='none', alpha=0.7, label='Terrain')\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2],\n",
    "    label=\"UAV Trajectory\", color=\"blue\")\n",
    "\n",
    "# Plot the UAV's trajectory\n",
    "ax.plot(\n",
    "    trajectory[:, 0], trajectory[:, 1], trajectory[:, 2],\n",
    "    label=\"UAV Trajectory\", color=\"blue\"\n",
    ")\n",
    "\n",
    "# Add gray 'x' markers every 50 steps\n",
    "for i in range(0, len(trajectory), 50):\n",
    "    ax.scatter(\n",
    "        trajectory[i, 0], trajectory[i, 1], trajectory[i, 2],\n",
    "        color='gray', s=50, marker='x', label='Step Marker' if i == 0 else None\n",
    "    )\n",
    "\n",
    "\n",
    "ax.scatter(uav_x, uav_y, uav_z, color='red', s=100, label='Start Point', marker='o')\n",
    "ax.scatter(*goal_position, color=\"blue\", s=100, label=\"Goal\", marker='^')\n",
    "\n",
    "cmap = colormaps.get_cmap(\"tab10\")  # Retrieves the \"tab10\" colormap\n",
    "for i, obs in enumerate(obstacle_positions):\n",
    "    color = cmap(i)  # Get a unique color from the colormap\n",
    "    ax.scatter(*obs, color=color, label=f\"Obstacle {i+1}\", s=100)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"X Dimension\")\n",
    "ax.set_ylabel(\"Y Dimension\")\n",
    "ax.set_zlabel(\"Elevation (km)\")\n",
    "ax.set_title(\"UAV Trajectory Over Smoothed Terrain\")\n",
    "ax.legend()\n",
    "\n",
    "# Set viewing angle\n",
    "ax.view_init(elev=30, azim=80)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Plot Repulsive Forces for Each Obstacle\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, obs in enumerate(obstacle_positions):\n",
    "    plt.plot(range(steps), F_rep_individual[i], label=f\"Repulsive Force from Obstacle {i+1}\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Repulsive Force Magnitude\")\n",
    "plt.title(\"Repulsive Forces from Each Obstacle\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Rewards Over Steps\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(rewards)), rewards, label=\"Reward\", color=\"purple\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward vs Steps\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0eeb2-594a-4206-9c96-98219c0cb50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308adf41-2c73-4c85-8b33-668e954f4a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1e5bd-1686-4ed2-b438-bf0cf8f28fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dac52a-4105-4455-a7ed-5229abd0e068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
